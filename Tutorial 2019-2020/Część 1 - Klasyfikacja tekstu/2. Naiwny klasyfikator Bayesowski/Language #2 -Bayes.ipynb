{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Language #2 -Bayes.ipynb","provenance":[{"file_id":"1pGXCJ6hpXwLNZPLfQ3cMNyYLybXuW6WF","timestamp":1577724013941}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bKWJbvwXfs7Y","colab_type":"text"},"source":["Moja wersja Naiwnego Algorytmu Bayesa\n","\n","Ten program rozpoznaje język danego dokumentu opierając się o listy częstotliwości słów w danych językach spośród 50 000 najpopularniejszych w każdym z nich (bazowane na www.opensubtitles.org z opracowania https://invokeit.wordpress.com/frequency-word-lists/).\n","\n","Z powodów zapisu dokument i listy słów muszą mieć ten sam format zapisu, najlepiej UTF-8.\n","\n","Dokument do sprawdzenia należy nazwać documnt.txt, a pliki ze słownikami należy nazwać nazwa_języka.txt i umieścić w folderze o nazwie dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6Ec3jc4981ew","colab":{}},"source":["import glob\n","import numpy as np\n","\n","path = '/content/dataset'\n","\n","files = [f for f in glob.glob(path + \"**/*.txt\", recursive=True)]\n","\n","dataset_languages = []\n","dataset = []\n","\n","for languageFile in files:\n","  with open(languageFile, 'r') as openFile:\n","    file_text = openFile.read(10**9)\n","    file_texts = file_text.split(\"\\n\")\n","    mono_dataset = []\n","    for el in file_texts:\n","      if(len(el.split(\" \"))<2):\n","        continue\n","      mono_dataset.append([el.split(\" \")[0],int(el.split(\" \")[1])])\n","    dataset_languages.append(languageFile[:-4].replace(\"/content/dataset/\",\"\"))\n","    dataset.append(mono_dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhgJ507SNQYj","colab_type":"code","colab":{}},"source":["dataset_occurencies = []\n","for el in dataset:\n","  suma = 0\n","  dictionary = {}\n","  for entry in el:\n","    suma+=entry[1]\n","  for entry in el:\n","    dictionary[entry[0]] = (entry[1]/suma)\n","  dataset_occurencies.append(dictionary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NQvPCofH81MA","colab":{}},"source":["document_path = '/content/document.txt'\n","\n","with open(document_path, 'r') as doc:\n","  doc_text = doc.read(10**9)\n","\n","for zn in (\",.<>/?;:\\\\|{{()}}[]-_=+\"):\n","  doc_text = doc_text.replace(zn,\"\")\n","doc_text = doc_text[1:]\n","doc_text_words = doc_text.split(\" \")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2dvIrw2KnVu","colab_type":"code","colab":{}},"source":["probabilities = [1.0 for data in dataset]\n","\n","for word in doc_text_words:\n","  word = word.lower()\n","  word = word.replace(' ','')\n","  suma = 0\n","  for c_class in dataset_occurencies:\n","    if(word in c_class):\n","      suma+=c_class[word]\n","    suma+=1\n","  i=0\n","  for c_class in dataset_occurencies:\n","    if(word in c_class):\n","      probabilities[i]+=np.log((c_class[word]+1)/suma)\n","    else:\n","      probabilities[i]+=np.log(1/suma)\n","    i+=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"6a01121c-121b-4754-f195-2e6e8b299d0d","executionInfo":{"status":"ok","timestamp":1577738193240,"user_tz":-60,"elapsed":9171,"user":{"displayName":"Dawid Ratyński","photoUrl":"","userId":"16936771803985771693"}},"id":"Y3tseE4XTjMy","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(dataset_languages[probabilities.index(np.max(probabilities))])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["en\n"],"name":"stdout"}]}]}